{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|  86.15264166176489|\n",
      "| 130.90262431531178|\n",
      "| 131.87347645926954|\n",
      "| 108.90273592781455|\n",
      "| 106.14318415977431|\n",
      "| 106.09883267192475|\n",
      "| 40.132207438047345|\n",
      "| -84.26511873837907|\n",
      "|-237.24335913359255|\n",
      "|-100.03683886688424|\n",
      "| -28.98353646869765|\n",
      "|-18.983003541644592|\n",
      "| -52.26821970150493|\n",
      "| -39.33952316107769|\n",
      "|-11.889556641492788|\n",
      "| -8.369444259110281|\n",
      "| -78.82579151690388|\n",
      "| -263.1258736080613|\n",
      "| -200.2852960602999|\n",
      "| -89.95247700274817|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "('meanAbsoluteError %2.4f', 75.2524435763989)\n",
      "('meanSquaredError %2.4f', 10320.728049367128)\n",
      "('rootMeanSquaredError %2.4f', 101.59098409488476)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from matplotlib.pyplot import *\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.regression  import LinearRegression\n",
    "\n",
    "path=\"./dataset/bike/hour.csv\"\n",
    "test = spark.read.format(\"CSV\").option(\"header\",\"true\").load(path)\n",
    "test = test.drop(\"instant\",\"dteday\",\"casual\",\"registered\")\n",
    "colums_cat = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
    "stringindexer_stages = [StringIndexer(inputCol=c, outputCol='stringindexed_' + c) for c in colums_cat]\n",
    "onehotencoder_stages = [OneHotEncoder(inputCol='stringindexed_' + c, outputCol='onehotencoded_' + c) for c in colums_cat]\n",
    "all_stages = stringindexer_stages + onehotencoder_stages\n",
    "reg_pipline = Pipeline(stages=all_stages)\n",
    "reg_pip_model = reg_pipline.fit(test)\n",
    "reg_test = reg_pip_model.transform(test)\n",
    "reg_feature_columns = ['onehotencoded_' + c for c in colums_cat] + ['temp', 'atemp', 'hum','windspeed']\n",
    "reg_labels_columns = ['cnt']\n",
    "reg_test = reg_test.withColumn(\"temp\",reg_test.temp.cast(DoubleType()))\n",
    "reg_test = reg_test.withColumn(\"atemp\",reg_test.atemp.cast(DoubleType()))\n",
    "reg_test = reg_test.withColumn(\"hum\",reg_test.hum.cast(DoubleType()))\n",
    "reg_test = reg_test.withColumn(\"windspeed\",reg_test.windspeed.cast(DoubleType()))\n",
    "reg_test = reg_test.withColumn(\"cnt\",reg_test.cnt.cast(DoubleType()))\n",
    "reg_test = reg_test.select(reg_feature_columns + reg_labels_columns)\n",
    "temp_columns = reg_test.columns[0:12];\n",
    "vectorassembler = VectorAssembler(inputCols=temp_columns, outputCol='features')\n",
    "reg_test =vectorassembler.transform(reg_test)\n",
    "reg_test = reg_test.select(['features'] + reg_labels_columns)\n",
    "linReg = LinearRegression(labelCol=\"cnt\", featuresCol=\"features\",maxIter=1000, regParam=0.3, solver=\"sgd\")\n",
    "linReg_model = linReg.fit(reg_test)\n",
    "reg_train_summary = linReg_model.summary\n",
    "reg_train_summary.residuals.show()\n",
    "print(\"meanAbsoluteError %2.4f\",reg_train_summary.meanAbsoluteError)\n",
    "print(\"meanSquaredError %2.4f\",reg_train_summary.meanSquaredError)\n",
    "print(\"rootMeanSquaredError %2.4f\",reg_train_summary.rootMeanSquaredError)\n",
    "#pylab inline\n",
    "#targets_count = reg_test.select('cnt').count();\n",
    "#targets = reg_test.select('cnt').take(10)\n",
    "#col = ['b' for x in range(10)]\n",
    "#hist(targets, bins=40, color=col, normed=True)\n",
    "#fig = matplotlib.pyplot.gcf()\n",
    "#fig.set_size_inches(16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 31.2489\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from matplotlib.pyplot import *\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "path=\"./dataset/bike/hour.csv\"\n",
    "test = spark.read.format(\"CSV\").option(\"header\",\"true\").load(path)\n",
    "test = test.drop(\"instant\",\"dteday\",\"casual\",\"registered\")\n",
    "colums_cat = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
    "for cul in colums_cat:\n",
    "    test = test.withColumn(cul, test[cul].cast(DoubleType()))\n",
    "test = test.withColumn('cnt', test['cnt'].cast(DoubleType()))\n",
    "assembler = VectorAssembler(inputCols=colums_cat,outputCol=\"features\")\n",
    "dt_test = assembler.transform(test)\n",
    "dt_test = dt_test.select(['features','cnt']);\n",
    "dt = DecisionTreeRegressor(labelCol='cnt',maxDepth=20, maxBins=100)\n",
    "dt_model = dt.fit(dt_test)\n",
    "dt_predict = dt_model.transform(dt_test)\n",
    "#dt_predict = dt_predict.withColumn(\"prediction\", dt_predict.prediction)\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"cnt\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(dt_predict)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\n",
      "+-----------------------+-------+\n",
      "|features               |clicked|\n",
      "+-----------------------+-------+\n",
      "|[18.0,1.0,0.0,10.0,0.5]|1.0    |\n",
      "+-----------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "dataset = spark.createDataFrame(\n",
    "    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n",
    "    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(dataset)\n",
    "print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\n",
    "output.select(\"features\", \"clicked\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chose 351 categorical features: 645, 69, 365, 138, 101, 479, 333, 249, 0, 555, 666, 88, 170, 115, 276, 308, 5, 449, 120, 247, 614, 677, 202, 10, 56, 533, 142, 500, 340, 670, 174, 42, 417, 24, 37, 25, 257, 389, 52, 14, 504, 110, 587, 619, 196, 559, 638, 20, 421, 46, 93, 284, 228, 448, 57, 78, 29, 475, 164, 591, 646, 253, 106, 121, 84, 480, 147, 280, 61, 221, 396, 89, 133, 116, 1, 507, 312, 74, 307, 452, 6, 248, 60, 117, 678, 529, 85, 201, 220, 366, 534, 102, 334, 28, 38, 561, 392, 70, 424, 192, 21, 137, 165, 33, 92, 229, 252, 197, 361, 65, 97, 665, 583, 285, 224, 650, 615, 9, 53, 169, 593, 141, 610, 420, 109, 256, 225, 339, 77, 193, 669, 476, 642, 637, 590, 679, 96, 393, 647, 173, 13, 41, 503, 134, 73, 105, 2, 508, 311, 558, 674, 530, 586, 618, 166, 32, 34, 148, 45, 161, 279, 64, 689, 17, 149, 584, 562, 176, 423, 191, 22, 44, 59, 118, 281, 27, 641, 71, 391, 12, 445, 54, 313, 611, 144, 49, 335, 86, 672, 172, 113, 681, 219, 419, 81, 230, 362, 451, 76, 7, 39, 649, 98, 616, 477, 367, 535, 103, 140, 621, 91, 66, 251, 668, 198, 108, 278, 223, 394, 306, 135, 563, 226, 3, 505, 80, 167, 35, 473, 675, 589, 162, 531, 680, 255, 648, 112, 617, 194, 145, 48, 557, 690, 63, 640, 18, 282, 95, 310, 50, 67, 199, 673, 16, 585, 502, 338, 643, 31, 336, 613, 11, 72, 175, 446, 612, 143, 43, 250, 231, 450, 99, 363, 556, 87, 203, 671, 688, 104, 368, 588, 40, 304, 26, 258, 390, 55, 114, 171, 139, 418, 23, 8, 75, 119, 58, 667, 478, 536, 82, 620, 447, 36, 168, 146, 30, 51, 190, 19, 422, 564, 305, 107, 4, 136, 506, 79, 195, 474, 664, 532, 94, 283, 395, 332, 528, 644, 47, 15, 163, 200, 68, 62, 277, 691, 501, 90, 111, 254, 227, 337, 122, 83, 309, 560, 639, 676, 222, 592, 364, 100\n",
      "+-----+--------------------+--------------------+\n",
      "|label|            features|             indexed|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "data = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "indexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexed\", maxCategories=10)\n",
    "indexerModel = indexer.fit(data)\n",
    "\n",
    "categoricalFeatures = indexerModel.categoryMaps\n",
    "print(\"Chose %d categorical features: %s\" %\n",
    "      (len(categoricalFeatures), \", \".join(str(k) for k in categoricalFeatures.keys())))\n",
    "\n",
    "# Create new column \"indexed\" with categorical values transformed to indices\n",
    "indexedData = indexerModel.transform(data)\n",
    "indexedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+\n",
      "|label|weight| features|\n",
      "+-----+------+---------+\n",
      "|  1.0|   2.0|    [1.0]|\n",
      "|  0.0|   2.0|(1,[],[])|\n",
      "+-----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame([(1.0, 2.0, Vectors.dense(1.0)),(0.0, 2.0, Vectors.sparse(1, [], []))], [\"label\", \"weight\", \"features\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
